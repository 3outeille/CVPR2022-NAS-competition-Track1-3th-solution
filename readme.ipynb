{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prior-Guided Neural Architecture Search \n",
    "\n",
    "Competition Homepage: [2022 CVPR Track1: SuperNet Track](https://aistudio.baidu.com/aistudio/competition/detail/149/0/introduction)\n",
    "\n",
    "Table of Content\n",
    "\n",
    "- Environment Setup\n",
    "- SuperNet Training\n",
    "- SuperNet Evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Environment Setup\n",
    "\n",
    "\n",
    "1.1 Env Requirements\n",
    "\n",
    "```\n",
    "paddle==2.2.2\n",
    "fire\n",
    "matplotlib\n",
    "visualdl \n",
    "```\n",
    "\n",
    "You can run commands:\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt \n",
    "```\n",
    "\n",
    "1.2 Data Prepare \n",
    "\n",
    "You can download imagenet-mini [from kaggle](https://www.kaggle.com/datasets/ifigotin/imagenetmini-1000). \n",
    "\n",
    "ImageNet-mini is a toy dataset compared with imagenet-1k, which has 1000 classes and is only 4.24GB.\n",
    "\n",
    "For quick iteration, we adopted imagenet-mini as a dataset for training and evaluation. If you have enough computation resources, you can try imagine-1k directly.\n",
    "\n",
    "In PP AI Studio, [imagenet-mini datasets](https://aistudio.baidu.com/aistudio/datasetdetail/89857) have been offered to us. \n",
    "\n",
    "\n",
    "1.3 Arch Json Prepare \n",
    "\n",
    "[Here](https://aistudio.baidu.com/aistudio/datasetdetail/134077) you can get the 4,5000 architectures for test. Download CVPR_2022_NAS_Track1_test.json and put it to `checkpoints/CVPR_2022_NAS_Track1_test.json` \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. SuperNet Training\n",
    "\n",
    "In this part, we will introduce the overall flow of training.\n",
    "\n",
    "(1) Import necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start due to the missing module 'decorator'. Consider installing this module.\n",
      "Click <a href='https://aka.ms/kernelFailuresMissingModule'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import paddle\n",
    "import paddle.nn as nn\n",
    "from paddle.nn import CrossEntropyLoss\n",
    "from paddle.vision.transforms import (\n",
    "    RandomHorizontalFlip, RandomResizedCrop, SaturationTransform, \n",
    "    Compose, Resize, HueTransform, BrightnessTransform, ContrastTransform, \n",
    "    RandomCrop, Normalize, RandomRotation, CenterCrop)\n",
    "from paddle.io import DataLoader\n",
    "from paddle.optimizer.lr import CosineAnnealingDecay, MultiStepDecay, LinearWarmup\n",
    "\n",
    "from hnas.utils.callbacks import LRSchedulerM, MyModelCheckpoint\n",
    "from hnas.utils.transforms import ToArray\n",
    "from hnas.dataset.random_size_crop import MyRandomResizedCrop\n",
    "from paddle.vision.datasets import DatasetFolder\n",
    "\n",
    "from paddleslim.nas.ofa.convert_super import Convert, supernet\n",
    "from paddleslim.nas.ofa import RunConfig, DistillConfig, ResOFA\n",
    "from paddleslim.nas.ofa.utils import utils\n",
    "\n",
    "import paddle.distributed as dist\n",
    "from hnas.utils.yacs import CfgNode\n",
    "from hnas.models.builder import build_classifier\n",
    "from hnas.utils.hapi_wrapper import Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) Set the loss function and accuracy tools.\n",
    "\n",
    "We offer three ways to compute loss:\n",
    "\n",
    "- Normal CrossEntropy Loss for teacher network.  \n",
    "- Inplace Distillation for student network.\n",
    "- Knowledge Distillation for student network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _loss_forward(self, input, tea_input=None, label=None):\n",
    "    if tea_input is not None and label is not None:\n",
    "        # knoledge distillation = cross entropy + inplace distillation\n",
    "        ce = paddle.nn.functional.cross_entropy(\n",
    "            input,\n",
    "            label,\n",
    "            weight=self.weight,\n",
    "            ignore_index=self.ignore_index,\n",
    "            reduction=self.reduction,\n",
    "            soft_label=self.soft_label,\n",
    "            axis=self.axis,\n",
    "            name=self.name)\n",
    "\n",
    "        kd = paddle.nn.functional.cross_entropy(\n",
    "            input,\n",
    "            paddle.nn.functional.softmax(tea_input),\n",
    "            weight=self.weight,\n",
    "            ignore_index=self.ignore_index,\n",
    "            reduction=self.reduction,\n",
    "            soft_label=True,\n",
    "            axis=self.axis)\n",
    "        return ce, kd\n",
    "    elif tea_input is not None and label is None:\n",
    "        # inplace distillation\n",
    "        kd = paddle.nn.functional.cross_entropy(\n",
    "            input,\n",
    "            paddle.nn.functional.softmax(tea_input),\n",
    "            weight=self.weight,\n",
    "            ignore_index=self.ignore_index,\n",
    "            reduction=self.reduction,\n",
    "            soft_label=True,\n",
    "            axis=self.axis)\n",
    "        return kd \n",
    "    elif label is not None:\n",
    "        # normal cross entropy \n",
    "        ce = paddle.nn.functional.cross_entropy(\n",
    "            input,\n",
    "            label,\n",
    "            weight=self.weight,\n",
    "            ignore_index=self.ignore_index,\n",
    "            reduction=self.reduction,\n",
    "            soft_label=False,\n",
    "            axis=self.axis,\n",
    "            name=self.name)\n",
    "        return ce\n",
    "    else:\n",
    "        raise \"Not Implemented Loss.\"\n",
    "\n",
    "CrossEntropyLoss.forward = _loss_forward\n",
    "\n",
    "def _compute(self, pred, tea_pred, label=None, *args):\n",
    "    if label is None:\n",
    "        label = tea_pred\n",
    "    pred = paddle.argsort(pred, descending=True)\n",
    "    pred = paddle.slice(\n",
    "        pred, axes=[len(pred.shape) - 1], starts=[0], ends=[self.maxk])\n",
    "    if (len(label.shape) == 1) or \\\n",
    "        (len(label.shape) == 2 and label.shape[-1] == 1):\n",
    "        label = paddle.reshape(label, (-1, 1))\n",
    "    elif label.shape[-1] != 1:\n",
    "        label = paddle.argmax(label, axis=-1, keepdim=True)\n",
    "    correct = pred == label\n",
    "    return paddle.cast(correct, dtype='float32')\n",
    "\n",
    "paddle.metric.Accuracy.compute = _compute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) Hyperparameter Settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone='resnet48_prelu'\n",
    "image_size='224'\n",
    "max_epoch=70\n",
    "lr=0.001\n",
    "weight_decay=0.\n",
    "momentum=0.9\n",
    "batch_size=256\n",
    "dyna_batch_size=4\n",
    "warmup=2\n",
    "phase=None\n",
    "resume=None\n",
    "pretrained='checkpoints/resnet48.pdparams'\n",
    "image_dir='/root/paddlejob/workspace/env_run/data/ILSVRC2012/'\n",
    "save_dir='checkpoints/res48-depth'\n",
    "save_freq=20\n",
    "log_freq=100\n",
    "visualdl_dir=\"./visualdl_log/autoslim3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4) Run and Main function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(\n",
    "    backbone='resnet48',\n",
    "    image_size='224',\n",
    "    max_epoch=120,\n",
    "    lr=0.0025,\n",
    "    weight_decay=0.,\n",
    "    momentum=0.9,\n",
    "    batch_size=80,\n",
    "    dyna_batch_size=4,\n",
    "    warmup=2,\n",
    "    phase=None,\n",
    "    resume=None,\n",
    "    pretrained='checkpoints/resnet48.pdparams',\n",
    "    image_dir='/root/paddlejob/workspace/env_run/data/ILSVRC2012/',\n",
    "    save_dir='checkpoints/res48-depth',\n",
    "    save_freq=20,\n",
    "    log_freq=100,\n",
    "    visualdl_dir=\"./visualdl_log/autoslim3\",\n",
    "    **kwargs\n",
    "    ):\n",
    "    run_config = locals()\n",
    "    run_config.update(run_config[\"kwargs\"])\n",
    "    del run_config[\"kwargs\"]\n",
    "    config = CfgNode(run_config)\n",
    "    config.image_size_list = [int(x) for x in config.image_size.split(',')]\n",
    "\n",
    "    nprocs = len(paddle.get_cuda_rng_state())\n",
    "    gpu_str = []\n",
    "    for x in range(nprocs):\n",
    "        gpu_str.append(str(x))\n",
    "    gpu_str = ','.join(gpu_str)\n",
    "    print(f'gpu num: {nprocs}')\n",
    "    dist.spawn(main, args=(config,), nprocs=nprocs, gpus=gpu_str)\n",
    "\n",
    "\n",
    "def main(cfg):\n",
    "    paddle.set_device('gpu:{}'.format(dist.ParallelEnv().device_id))\n",
    "    if dist.get_rank() == 0:\n",
    "        print(cfg)\n",
    "    IMAGE_MEAN = (0.485,0.456,0.406)\n",
    "    IMAGE_STD = (0.229,0.224,0.225)\n",
    "\n",
    "    cfg.lr = cfg.lr * cfg.batch_size * dist.get_world_size() / 256\n",
    "    warmup_step = int(1281024 / (cfg.batch_size * dist.get_world_size())) * cfg.warmup\n",
    "\n",
    "    # data augmentation \n",
    "    transforms = Compose([\n",
    "        MyRandomResizedCrop(cfg.image_size_list),\n",
    "        RandomHorizontalFlip(),\n",
    "        ToArray(),\n",
    "        Normalize(IMAGE_MEAN, IMAGE_STD),\n",
    "    ])\n",
    "    train_set = DatasetFolder(os.path.join(cfg.image_dir, 'train'), transform=transforms)\n",
    "    callbacks = [LRSchedulerM(), \n",
    "                 MyModelCheckpoint(cfg.save_freq, cfg.save_dir, cfg.resume, cfg.phase),\n",
    "                 paddle.callbacks.VisualDL(log_dir=cfg.visualdl_dir)]\n",
    "\n",
    "    # build resnet48 and teacher net\n",
    "    net = build_classifier(cfg.backbone, pretrained=cfg.pretrained, reorder=True)\n",
    "    tnet = build_classifier(cfg.backbone, pretrained=cfg.pretrained, reorder=False)\n",
    "    origin_weights = {}\n",
    "    for name, param in net.named_parameters():\n",
    "        origin_weights[name] = param\n",
    "    \n",
    "    # convert resnet48 to supernet \n",
    "    sp_model = Convert(supernet(expand_ratio=[1.0])).convert(net)  # net转换成supernet\n",
    "    utils.set_state_dict(sp_model, origin_weights)  # 重新对supernet加载数据\n",
    "    del origin_weights\n",
    "\n",
    "    # set candidate config \n",
    "    cand_cfg = {\n",
    "            'i': [224],  # image size\n",
    "            'd': [(2, 5), (2, 5), (2, 8), (2, 5)],  # depth\n",
    "            'k': [3],  # kernel size\n",
    "            'c': [1.0, 0.95, 0.9, 0.85, 0.8, 0.75, 0.7] # channel ratio\n",
    "    }\n",
    "\n",
    "    default_distill_config = {\n",
    "        'lambda_distill': 0.5,\n",
    "        'teacher_model': tnet,\n",
    "        'mapping_layers': None,\n",
    "        'teacher_model_path': None,\n",
    "        'distill_fn': None,\n",
    "        'mapping_op': 'conv2d'\n",
    "    }\n",
    "\n",
    "    ofa_net = ResOFA(sp_model,\n",
    "                     distill_config=DistillConfig(**default_distill_config), \n",
    "                     candidate_config=cand_cfg,\n",
    "                     block_conv_num=2)\n",
    "\n",
    "    # ofa_net.set_task(['depth', 'expand_ratio'])\n",
    "    ofa_net.set_task('expand_ratio')\n",
    "\n",
    "    run_config = {'dynamic_batch_size': cfg.dyna_batch_size}\n",
    "    model = Trainer(ofa_net, cfg=run_config)\n",
    "\n",
    "    # calculate loss by ce \n",
    "    model.prepare(\n",
    "        paddle.optimizer.Momentum(\n",
    "            learning_rate=LinearWarmup( # delete cfg.lr * 0.05 \n",
    "                CosineAnnealingDecay(cfg.lr, cfg.max_epoch), warmup_step, 0., cfg.lr),\n",
    "            momentum=cfg.momentum,\n",
    "            parameters=model.parameters(),\n",
    "            weight_decay=cfg.weight_decay),\n",
    "        CrossEntropyLoss(),\n",
    "        paddle.metric.Accuracy(topk=(1,5)))\n",
    "        \n",
    "    model.fit(\n",
    "        train_set,\n",
    "        None,\n",
    "        epochs=cfg.max_epoch,\n",
    "        batch_size=cfg.batch_size,\n",
    "        save_dir=cfg.save_dir,\n",
    "        save_freq=cfg.save_freq,\n",
    "        log_freq=cfg.log_freq,\n",
    "        shuffle=True,\n",
    "        num_workers=8,\n",
    "        verbose=2, \n",
    "        drop_last=True,\n",
    "        callbacks=callbacks,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(5) Start to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(backbone=backbone, \n",
    "    image_size=image_size,\n",
    "    max_epoch=max_epoch,\n",
    "    lr=lr,\n",
    "    weight_decay=weight_decay,\n",
    "    momentum=momentum,\n",
    "    batch_size=batch_size,\n",
    "    dyna_batch_size=dyna_batch_size,\n",
    "    warmup=warmup,\n",
    "    pretrained=pretrained,\n",
    "    image_dir=image_dir,\n",
    "    save_dir=save_dir,\n",
    "    visualdl_dir=visualdl_dir,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous section did not cover the core implementation. In the next section we will describe how `RANK LOSS` is built.\n",
    "\n",
    "Rank Loss Function: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import paddle.nn as nn\n",
    "import paddle.nn.functional as F\n",
    "\n",
    "class PairwiseRankLoss(nn.Layer):\n",
    "    \"\"\"pairwise ranking loss for rank consistency \n",
    "\n",
    "    Args:\n",
    "        prior1 (float | int): the prior value of arch1 \n",
    "        prior2 (float | int): the prior value of arch2 \n",
    "        loss1: the batch loss of arch1 \n",
    "        loss2: the batch loss of arch2 \n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, prior1, prior2, loss1, loss2, coeff=1.):\n",
    "        return coeff * F.relu(loss2-loss1.detach()) if prior1 < prior2 else coeff * F.relu(loss1.detach()-loss2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Integrate Pairwise `Rank Loss` into the `Sandwich Rule`. For example, take flops as prior.\n",
    "\n",
    "```python\n",
    "def train_batch_sandwich_with_rank(self, inputs, labels=None, **kwargs):\n",
    "    assert self.model._optimizer, \"model not ready, please call `model.prepare()` first\"\n",
    "    self.model.network.model.train() # set network to training mode.\n",
    "    self.mode = 'train'\n",
    "\n",
    "    inputs = to_list(inputs)\n",
    "    self._input_info = _update_input_info(inputs)\n",
    "    labels = to_variable(labels).squeeze(0)\n",
    "    epoch = kwargs.get('epoch', None)\n",
    "    self.epoch = epoch\n",
    "    nBatch = kwargs.get('nBatch', None)\n",
    "    step = kwargs.get('step', None)\n",
    "\n",
    "    subnet_seed = int('%d%.1d' % (epoch * nBatch + step, step)) # set seed \n",
    "    np.random.seed(subnet_seed)\n",
    "\n",
    "    ######### Sandwich Rule ##############\n",
    "\n",
    "    # sample largest subnet as teacher network\n",
    "    largest_config = self.model.network.active_autoslim_subnet(sample_type=\"largest\")\n",
    "    self.model.network.set_net_config(largest_config)\n",
    "    if self._nranks > 1:\n",
    "        teacher_output = self.ddp_model.forward(*[to_variable(x) for x in inputs])\n",
    "    else:\n",
    "        teacher_output = self.model.network.forward(*[to_variable(x) for x in inputs])\n",
    "    # normal forward with CrossEntropy. \n",
    "    loss1 = self.model._loss(input=teacher_output[0], tea_input=None, label=labels)\n",
    "    loss1.backward()\n",
    "\n",
    "    # sample smallest subnet as student network and perform distill operation\n",
    "    smallest_config = self.model.network.active_autoslim_subnet(sample_type=\"smallest\")\n",
    "    self.model.network.set_net_config(smallest_config)\n",
    "    if self._nranks > 1:\n",
    "        output = self.ddp_model.forward(*[to_variable(x) for x in inputs])\n",
    "    else:\n",
    "        output = self.model.network.forward(*[to_variable(x) for x in inputs])\n",
    "    # forward with inplace distillation\n",
    "    loss2 = self.model._loss(input=output[0],tea_input=teacher_output[0], label=None)\n",
    "    loss2.backward()\n",
    "    del output \n",
    "\n",
    "    # sample random subnets as student net and perform distill operation\n",
    "    for _ in range(self.dyna_bs-2): \n",
    "        random_config1 = self.model.network.active_autoslim_subnet(sample_type=\"random\")\n",
    "        self.model.network.set_net_config(random_config1)\n",
    "        if self._nranks > 1:\n",
    "            output = self.ddp_model.forward(*[to_variable(x) for x in inputs])\n",
    "        else:\n",
    "            output = self.model.network.forward(*[to_variable(x) for x in inputs])\n",
    "        # forward with inplace distillation\n",
    "        loss = self.model._loss(input=output[0],tea_input=teacher_output[0], label=None)\n",
    "        loss.backward()\n",
    "        del output\n",
    "\n",
    "    # pairwise rank loss \n",
    "    for _ in range(self.dyna_bs-2):\n",
    "        # generate two random arch to compute pairwise rank loss.\n",
    "        random_config1 = self.model.network.active_autoslim_subnet(sample_type=\"random\")\n",
    "        self.model.network.set_net_config(random_config1)\n",
    "        flops1 = get_arch_flops(self.model.network.gen_subnet_code)\n",
    "        if self._nranks > 1:\n",
    "            output = self.ddp_model.forward(*[to_variable(x) for x in inputs])\n",
    "        else:\n",
    "            output = self.model.network.forward(*[to_variable(x) for x in inputs])\n",
    "        loss3 = self.model._loss(input=output[0],tea_input=None, label=labels)\n",
    "        del output\n",
    "\n",
    "        random_config2 = self.model.network.active_autoslim_subnet(sample_type=\"random\")\n",
    "        self.model.network.set_net_config(random_config2)\n",
    "        flops2 = get_arch_flops(self.model.network.gen_subnet_code)\n",
    "        if self._nranks > 1:\n",
    "            output = self.ddp_model.forward(*[to_variable(x) for x in inputs])\n",
    "        else:\n",
    "            output = self.model.network.forward(*[to_variable(x) for x in inputs])\n",
    "        loss4 = self.model._loss(input=output[0],tea_input=None, label=labels)\n",
    "        \n",
    "        # gradually increase rank loss weights.\n",
    "        loss5 =  min(2, epoch/10.) * self.pairwise_rankloss(flops1, flops2, loss3, loss4)\n",
    "        loss5.backward()\n",
    "\n",
    "    self.model._optimizer.step()\n",
    "    self.model._optimizer.clear_grad()\n",
    "\n",
    "    metrics = []\n",
    "    for metric in self.model._metrics:\n",
    "        metric_outs = metric.compute(output[0], labels)\n",
    "        m = metric.update(*[to_numpy(m) for m in to_list(metric_outs)])\n",
    "        metrics.append(m)\n",
    "\n",
    "    return ([to_numpy(l) for l in [loss1]], metrics) if len(metrics) > 0 else [to_numpy(l) for l in [loss1]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. SubNet Evaluation\n",
    "\n",
    "For evaluation, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import paddle\n",
    "import paddle.distributed as dist\n",
    "from paddle.nn import CrossEntropyLoss\n",
    "from paddle.optimizer.lr import CosineAnnealingDecay, LinearWarmup\n",
    "from paddle.vision.datasets import DatasetFolder\n",
    "from paddle.vision.transforms import CenterCrop, Compose, Normalize, Resize\n",
    "\n",
    "from hnas.models.builder import build_classifier\n",
    "from hnas.utils.callbacks import EvalCheckpoint\n",
    "from hnas.utils.transforms import ToArray\n",
    "from hnas.utils.yacs import CfgNode\n",
    "from paddleslim.nas.ofa import DistillConfig, ResOFA\n",
    "from paddleslim.nas.ofa.convert_super import Convert, supernet\n",
    "from paddleslim.nas.ofa.utils import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone='resnet48_prelu'\n",
    "image_size='224'\n",
    "max_epoch=70\n",
    "lr=0.001\n",
    "weight_decay=0.\n",
    "momentum=0.9\n",
    "batch_size=256\n",
    "dyna_batch_size=4\n",
    "warmup=2\n",
    "phase=None\n",
    "resume=None\n",
    "pretrained='checkpoints/resnet48.pdparams'\n",
    "image_dir='/root/paddlejob/workspace/env_run/data/ILSVRC2012/'\n",
    "save_dir='checkpoints/res48-depth'\n",
    "save_freq=20\n",
    "log_freq=100\n",
    "visualdl_dir=\"./visualdl_log/autoslim3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(\n",
    "    backbone='resnet48',\n",
    "    image_size='224',\n",
    "    max_epoch=120,\n",
    "    lr=0.0025,\n",
    "    weight_decay=3e-5,\n",
    "    momentum=0.9,\n",
    "    batch_size=80,\n",
    "    dyna_batch_size=4,\n",
    "    warmup=2,\n",
    "    phase=None,\n",
    "    resume=None,\n",
    "    pretrained='checkpoints/resnet48.pdparams',\n",
    "    image_dir='/root/paddlejob/workspace/env_run/data/ILSVRC2012/',\n",
    "    save_dir='checkpoints/res48-depth',\n",
    "    save_freq=5,\n",
    "    log_freq=100,\n",
    "    json_path=None,\n",
    "    **kwargs\n",
    "    ):\n",
    "    run_config = locals()\n",
    "    run_config.update(run_config[\"kwargs\"])\n",
    "    del run_config[\"kwargs\"]\n",
    "    config = CfgNode(run_config)\n",
    "    config.image_size_list = [int(x) for x in config.image_size.split(',')]\n",
    "\n",
    "    nprocs = len(paddle.get_cuda_rng_state())\n",
    "    gpu_str = []\n",
    "    for x in range(nprocs):\n",
    "        gpu_str.append(str(x))\n",
    "    gpu_str = ','.join(gpu_str)\n",
    "    print(f'gpu num: {nprocs}')\n",
    "    # dist.spawn(main, args=(config,), nprocs=nprocs, gpus=gpu_str)\n",
    "    main(config)\n",
    "\n",
    "\n",
    "def main(cfg):\n",
    "    paddle.set_device('gpu:{}'.format(dist.ParallelEnv().device_id))\n",
    "    if dist.get_rank() == 0:\n",
    "        print(cfg)\n",
    "    IMAGE_MEAN = (0.485,0.456,0.406)\n",
    "    IMAGE_STD = (0.229,0.224,0.225)\n",
    "\n",
    "    cfg.lr = cfg.lr * cfg.batch_size * dist.get_world_size() / 256\n",
    "    warmup_step = int(1281024 / (cfg.batch_size * dist.get_world_size())) * cfg.warmup\n",
    "\n",
    "    val_transforms = Compose([Resize(256), CenterCrop(224), ToArray(), Normalize(IMAGE_MEAN, IMAGE_STD)])\n",
    "    val_set = DatasetFolder(os.path.join(cfg.image_dir, 'val'), transform=val_transforms)\n",
    "    # val_set = HDF5DatasetFolder(\"/data/home/scv6681/run/data/hdf5/imagenetmini_val.h5\", transform=val_transforms)\n",
    "\n",
    "    eval_callbacks = [EvalCheckpoint('{}/final'.format(cfg.save_dir))]\n",
    "\n",
    "    net = build_classifier(cfg.backbone, pretrained=cfg.pretrained, reorder=True)\n",
    "    tnet = build_classifier(cfg.backbone, pretrained=cfg.pretrained, reorder=False)\n",
    "    origin_weights = {}\n",
    "    for name, param in net.named_parameters():\n",
    "        origin_weights[name] = param\n",
    "    \n",
    "    sp_model = Convert(supernet(expand_ratio=[1.0])).convert(net)  # net转换成supernet\n",
    "    utils.set_state_dict(sp_model, origin_weights)  # 重新对supernet加载数据\n",
    "    del origin_weights\n",
    "\n",
    "    cand_cfg = {\n",
    "            'i': [224],  # image size\n",
    "            'd': [(2, 5), (2, 5), (2, 8), (2, 5)],  # depth\n",
    "            'k': [3],  # kernel size\n",
    "            'c': [1.0, 0.95, 0.9, 0.85, 0.8, 0.75, 0.7] # channel ratio\n",
    "    }\n",
    "    ofa_net = ResOFA(sp_model,\n",
    "                     distill_config=DistillConfig(teacher_model=tnet), \n",
    "                     candidate_config=cand_cfg,\n",
    "                     block_conv_num=2)\n",
    "    ofa_net.set_task('expand_ratio')\n",
    "\n",
    "    run_config = {'dynamic_batch_size': cfg.dyna_batch_size}\n",
    "    model = Trainer(ofa_net, cfg=run_config)\n",
    "    model.prepare(\n",
    "        paddle.optimizer.Momentum(\n",
    "            learning_rate=LinearWarmup(\n",
    "                CosineAnnealingDecay(cfg.lr, cfg.max_epoch), warmup_step, 0., cfg.lr),\n",
    "            momentum=cfg.momentum,\n",
    "            parameters=model.parameters(),\n",
    "            weight_decay=cfg.weight_decay),\n",
    "        CrossEntropyLoss(),\n",
    "        paddle.metric.Accuracy(topk=(1,5)))\n",
    "\n",
    "    model.evaluate_whole_test(val_set, batch_size=cfg.batch_size, num_workers=8, callbacks=eval_callbacks, json_path=cfg.json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(backbone=backbone, \n",
    "    image_size=image_size,\n",
    "    max_epoch=max_epoch,\n",
    "    lr=lr,\n",
    "    weight_decay=weight_decay,\n",
    "    momentum=momentum,\n",
    "    batch_size=batch_size,\n",
    "    dyna_batch_size=dyna_batch_size,\n",
    "    warmup=warmup,\n",
    "    pretrained=pretrained,\n",
    "    image_dir=image_dir,\n",
    "    save_dir=save_dir,\n",
    "    visualdl_dir=visualdl_dir,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```python\n",
    "def evaluate_whole_test(\n",
    "        self,\n",
    "        eval_data,\n",
    "        batch_size=256,\n",
    "        log_freq=10,\n",
    "        verbose=1,\n",
    "        num_workers=4,\n",
    "        callbacks=None,\n",
    "        json_path=None):\n",
    "\n",
    "    candidate_path = json_path \n",
    "\n",
    "    with open(candidate_path, \"r\") as f:\n",
    "        candidate_dict = json.load(f)\n",
    "        save_candidate = candidate_dict.copy()\n",
    "\n",
    "    if eval_data is not None and isinstance(eval_data, Dataset):\n",
    "        eval_sampler = None \n",
    "        eval_loader = DataLoader(\n",
    "            eval_data, \n",
    "            batch_sampler=eval_sampler,\n",
    "            places=self._place,\n",
    "            shuffle=False, \n",
    "            num_workers=num_workers,\n",
    "            batch_size=batch_size, \n",
    "            return_list=True, \n",
    "            use_shared_memory=True,\n",
    "            use_buffer_reader=True)\n",
    "    else:\n",
    "        eval_loader = eval_data\n",
    "\n",
    "    self._test_dataloader = eval_loader\n",
    "\n",
    "    cbks = config_callbacks(\n",
    "        callbacks,\n",
    "        model=self,\n",
    "        log_freq=log_freq,\n",
    "        verbose=verbose,\n",
    "        metrics=self._metrics_name(), )\n",
    "\n",
    "    eval_steps = self._len_data_loader(eval_loader)\n",
    "\n",
    "    self.network.model.eval()\n",
    "\n",
    "    import time\n",
    "    show_flag = True\n",
    "\n",
    "    sample_result = []\n",
    "    for arch_name, config in candidate_dict.items():\n",
    "        s1 = time.time() \n",
    "        cbks.on_begin('eval', {'steps': eval_steps, 'metrics': self._metrics_name()})\n",
    "\n",
    "        self.network.active_specific_subnet(224, config['arch'])\n",
    "\n",
    "        logs = self._run_one_epoch(eval_loader, cbks, 'eval')\n",
    "        \n",
    "        s3 = time.time()\n",
    "        if ParallelEnv().local_rank == 0 and show_flag:\n",
    "            print(\"forward_one_epoch time: \", s3-s1)\n",
    "\n",
    "        cbks.on_end('eval', logs)\n",
    "\n",
    "        self._test_dataloader = None\n",
    "\n",
    "        eval_result = {}\n",
    "        for k in self._metrics_name():\n",
    "            eval_result[k] = logs[k]\n",
    "        sample_res = '{} {} {} {}'.format(arch_name, config['arch'], eval_result['acc_top1'], eval_result['acc_top5'])\n",
    "        if ParallelEnv().local_rank == 0:\n",
    "            print(sample_res)\n",
    "\n",
    "        sample_result.append(sample_res)\n",
    "\n",
    "        if ParallelEnv().local_rank == 0:\n",
    "            num = json_path.split('_')[-1].split(\".\")[0]\n",
    "            with open(f'checkpoints/results/19th_rkloss_mish_flops_latedecay_sandwich_2times/channel_sample_{num}.txt', 'a') as f:\n",
    "                f.write('{}\\n'.format(sample_res))\n",
    "\n",
    "        save_candidate[arch_name]['acc'] = eval_result['acc_top1']\n",
    "\n",
    "    if ParallelEnv().local_rank == 0:\n",
    "        save_path = candidate_path.replace('CVPR_2022_NAS_Track1_test', 'CVPR_2022_NAS_Track1_test_{}'.format(time.strftime(\"%Y_%m_%d__%H_%M_%S\", time.localtime())))\n",
    "        with open(save_path, 'w') as f:\n",
    "            json.dump(save_candidate, f)\n",
    "\n",
    "    return sample_result\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a950f88516a1a6ad48349b1bab432b2b283d21e009d9fef047f7a206edc26faa"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 ('pp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
